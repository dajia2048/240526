---
title: '谷歌再放核弹，开源最强大模型Gemini技术，碾压Llama 2，一月三大招硬刚OpenAI'
categories: ['36krai']
keywords: ['36krai']
date: Thu, 22 Feb 2024 01:26:07 GMT
lastmod: Thu, 22 Feb 2024 01:26:07 GMT
author: [['36krai']]
tags: ['36krai']
draft: false 
comments: true
reward: true 
mermaid: true 
showToc: true 
TocOpen: true 
hidemeta: false 
disableShare: true 
showbreadcrumbs: true 
cover:
    image: cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_d3c7915dd4ff4c68976327878f8fc118@000000_oswg336742oswg1080oswg611_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1
    alt: "'谷歌再放核弹，开源最强大模型Gemini技术，碾压Llama 2，一月三大招硬刚OpenAI'"
    relative: false
---

<div>

<div> 谷歌, 开源模型, Gemma, Llama 2, 竞争<br/>
技术进步，Gemma 发布，性能优越，挑战 Llama 2，基于 TPUv5e 芯片训练，可以在不同平台运行，性能突出，支持多种语言，强大的参数规模，在基准测试中击败 Llama 2<br/>
Gemma 与 Llama 2 对比，数学、代码能力突出，使用自家 TPUv5e 进行训练，分词器词表大小达到256k，适用于不同硬件平台<br/>
谷歌发布 Gemma 开源，连续三个大招，挑战 OpenAI 和 Meta，向生成式AI应用开发者带来福音，强化了AI原则，与英伟达合作优化模型<br/>
总结:<br/>谷歌发布 Gemma 开源，性能优越，挑战 Llama 2，技术进步突出，与英伟达合作优化模型，连续发布三个大招，向开源大模型铁王座进发，竞争激烈，挑战 OpenAI 和 Meta，对生成式AI应用开发者具有重大意义。 <div>
<p><strong>谷歌向最强开源大模型的宝座发起进攻</strong>！</p><p>智东西2月22日凌晨报道，昨日晚间，谷歌毫无预兆地发布了<strong>开源模型Gemma</strong>，直接狙击Llama 2，继通过Gemini拳打OpenAI后，试图用Gemma脚踢Meta。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_d3c7915dd4ff4c68976327878f8fc118@000000_oswg336742oswg1080oswg611_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲谷歌发布Gemma（图源：谷歌）</p><p>不同于Gemini的“全家桶”路线，Gemma主打<strong>轻量级</strong>、<strong>高性能</strong>，有<strong>20亿</strong>、<strong>70亿</strong>两种参数规模，能在笔记本电脑、台式机、物联网设备、移动设备和云端等不同平台运行。</p><p>性能方面，Gemma在18个基准测评中平均成绩击败目前的主流开源模型Llama 2和Mistral，特别是在<strong>数学</strong>、<strong>代码</strong>能力上表现突出，还直接<strong>登顶Hugging Face开源大模型排行榜</strong>。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_706f7f4da4d7482794c9906cfe997772@000000_oswg52215oswg1080oswg429_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲Gemma登顶Hugging Face开源大模型排行榜（图源：X）</p><p>谷歌同步放出了技术报告，通过深度解读，智东西注意到除了模型性能优异外，Gemma的分词器词表大小达到<strong>256k</strong>，这意味着它更容易扩展至其他语言。</p><p>谷歌还强调Gemma基于自家<strong>TPUv5e</strong>芯片训练，Gemma 7B使用了<strong>4096个</strong>TPUv5e，Gemma 2B使用了<strong>512个</strong>TPUv5e，秀出挑战英伟达GPU统治地位的“肌肉”。</p><p>短短12天，谷歌<strong>连续放出三个大招</strong>，先是<strong>9日</strong>宣布其最强大模型Gemini Ultra免费用，又在<strong>16日</strong>放出大模型“核弹”Gemini 1.5，再是<strong>21日</strong>突然放出开源模型Gemma，动作之密集、行动之迅速，似乎在向抢了自己风头的OpenAI宣战。</p><p>Gemma具体强在哪儿？它在哪些方面打赢了Llama 2？其技术原理和训练过程有哪些亮点？让我们从技术报告中寻找答案。</p><p><strong>Gemma官网地址：</strong></p><p>https://ai.google.dev/gemma</p><p><strong>Gemma开源地址：</strong></p><p>https://www.kaggle.com/models/google/gemma/code/</p><h2>01.采用Gemini相同架构，轻量级笔记本也能跑</h2><p>据介绍，Gemma模型的研发是<strong>受到Gemini的启发</strong>，它的名字来源于意大利语<strong>“宝石”</strong>，是由谷歌DeepMind和其他团队共同合作开发。</p><p>Gemma采用了与Gemini相同的技术和基础架构，基于英伟达GPU和谷歌云TPU等硬件平台进行优化，有20亿、70亿两种参数规模，每个规模又分<strong>预训练</strong>和<strong>指令微调</strong>两个版本。</p><p>性能方面，谷歌称Gemma在MMLU、BBH、HumanEval等<strong>八项</strong>基准测试集上大幅超过Llama 2。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_e6c508dde1924ab0815feacc928678d5@000000_oswg39820oswg901oswg675_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲Gemma在基准测试上与Llama 2的跑分对比（图源：谷歌）</p><p>在发布权重的同时，谷歌还推出Responsible Generative AI Toolkit等一系列工具，为使用Gemma创建更安全的AI应用程序提供指导。此外，谷歌通过原生Keras 3.0为JAX、PyTorch和TensorFlow等主要框架提供推理和监督微调（SFT）的工具链。</p><p>谷歌强调Gemma在设计时将其AI原则放在首位，通过大量微调和人类反馈强化学习（RLHF）使指令微调模型与负责任的行为对齐，还通过手工红队测试、自动对抗性测试等对模型进行评估。</p><p>此外，谷歌与英伟达宣布合作，利用英伟达TensorRT-LLM对Gemma进行优化。英伟达上周刚发布的聊天机器人Chat with RTX也将很快增加对Gemma的支持。</p><p>即日起，Gemma在全球范围内开放使用，用户可以在Kaggle、Hugging Face等平台上进行下载和试用，它可以直接在笔记本电脑或台式机上运行。</p><p>发布才几个小时，已有不少用户分享了试用体验。社交平台X用户@indigo11称其“速度飞快”，“输出很稳定”。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_ef7b472fceb54ccabf4c5328b9e99f94@000000_oswg59839oswg727oswg236_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲X用户@indigo11分享Gemma试用体验（图源：X）</p><p>还有用户尝试了其他语种，称Gemma对日语的支持很流畅。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_1d356f47897e405db5aab198963c7705@000000_oswg287334oswg730oswg699_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲X用户@AiXsatoshi分享Gemma在日语上的试用体验（图源：X）</p><h2>02.数学、代码能力碾压Llama 2，采用自家TPUv5e训练</h2><p>与Gemini发布时一样，谷歌此次也同步公开了Gemma的技术报告。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_9bcc457b30894fa292da62e1d1f67e5f@000000_oswg85972oswg762oswg419_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲Gemma技术报告（图源：谷歌）</p><p>报告称，Gemma 2B和7B模型分别在<strong>2T</strong>和<strong>6T</strong>的tokens上进行训练，数据主要来自网络文档、数学和代码的英语数据。不同于Gemini，这些模型不是多模态的，也没有针对多语言任务进行训练。</p><p>谷歌使用Gemini的SentencePiece分词器的一个子集以保证兼容性。它分割数字但不去除额外的空格，并且对未知标记依赖于字节级编码，词表大小为<strong>256k</strong>个tokens，这可能意味着它<strong>更容易扩展到其他语言</strong>。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_4437afd0a970439891088f7532ef737b@000000_oswg26905oswg586oswg184_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲开发者称256k分词器值得注意（图源：X）</p><p>两个规模中，70亿参数的Gemma 7B适用于GPU、TPU上的高效部署和开发，20亿参数的Gemma 2B则适用于CPU。</p><p>Gemma基于谷歌的开源模型和生态构建，包括Word2Vec、BERT、T5、T5X等，其模型架构基于<strong>Transformer</strong>，主要核心参数如下表。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_1255ffba59f6410f9d5bc64afdf12595@000000_oswg69337oswg576oswg643_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲Gemma模型主要参数（图源：谷歌）</p><p>在基准测评中，Gemma直接对标目前先进的开源模型Llama 2和Mistral，其中Gemma 7B在18个基准上取得<strong>11个</strong>优胜，并以平均分56.4高于同级别模型。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_3a726079a5294845967cc1959c02947b@000000_oswg148706oswg900oswg794_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲Gemma与Llama 2、Mistral基准测评分数对比（图源：谷歌）</p><p>从具体能力上看，Gemma 7B在问答、推理、数学/科学、代码等方面的标准学术基准测试平均分数都高于同规模的Llama 2和Mistral模型。</p><p>此外，其推理、数学/科学、代码能力还高于规模更大的Llama 2 13B。</p><p class="image-wrapper"><img src="cdn.g0f.cn/?r=https://www.36kr.com&url=https://img.36krcdn.com/hsossms/20240222/v2_5117c5e336174378872297f83d4c2183@000000_oswg39763oswg759oswg423_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">▲Gemma与Llama 2、Mistral各能力对比（图源：谷歌）</p><p>报告还详细介绍了Gemma训练采用的硬件：使用部署在256个芯片组成的Pod中的TPUv5e训练，这些Pod配置成一个16*16芯片的2D环形网络。</p><p>其中，Gemma 7B模型跨16个Pod进行训练，共使用了<strong>4096个</strong>TPUv5e；Gemma 2B模型跨越2个Pod进行训练，共使用了<strong>512个</strong>TPUv5e。</p><p>在一个Pod内部，谷歌为Gemma 7B使用了16路模型分片和16路数据复制，Gemma 2B则使用256路数据复制。优化器状态进一步通过类似于ZeRO-3的技术进行分片。</p><p><strong>技术报告地址：</strong></p><p>https://goo.gle/GemmaReport</p><h2>03.被OpenAI逼急了，谷歌一月连放三大招</h2><p>2024开年，OpenAI发布的Sora文生视频模型爆火，一举抢走了谷歌最新力作Gemini 1.5 Pro大模型的风头。</p><p>但谷歌并没有就此打住，而是乘胜追击放出一个月里的<strong>第三个大招</strong>，这三个大招分别是：</p><p>2月9日大年三十，谷歌宣布其最强大模型Gemini Ultra免费用，Gemini Ultra于2023年12月发布时在MMLU（大规模多任务语言理解）测评上超过人类专家，在32个多模态基准中取得30个SOTA（当前最优效果），几乎全方位超越GPT-4，<strong>向OpenAI发起强势一击。</strong>（《谷歌大年三十整大活！最强大模型Gemini Ultra免费用，狙击GPT-4》）</p><p>2月16日大年初七，谷歌放出其大模型核弹——Gemini 1.5，并将上下文窗口长度扩展到100万个tokens。Gemini 1.5 Pro可一次处理1小时的视频、11小时的音频、超过3万行代码或超过70万字的代码库，<strong>向OpenAI还没发布的GPT-5发起挑战。</strong>（《谷歌Gemini 1.5模型来了！突破100万个tokens，能处理1小时视频【附58页技术报告】》）</p><p>2月21日正月十二，谷歌在被“抢头条”后，一举将采用创建Gemini相同研究和技术的Gemma开源，一方面狙击Llama 2等开源模型，<strong>登上开源大模型铁王座</strong>，同时为嗷嗷待哺的生成式AI的应用开发者带来福音，更是为闭源的代表OpenAI狠狠地上了一课。</p><p>自2022年12月ChatGPT发布以来，AI领域扛把子谷歌就陷入被OpenAI压着打的境地，“复仇”心切。</p><p>在GPT-3大模型问世前，DeepMind的风头更胜一筹，坐拥AlphaGo、AlphaGo Zero、MuZero、AlphaFold等一系列打败人类的明星AI模型。随着生成式AI风口渐盛，谷歌DeepMind却开始显得力不从心，ChatGPT引发谷歌AI人才大军流向OpenAI，OpenAI却由此扶摇直上。</p><p>2023年3月，谷歌促成谷歌大脑和DeepMind冰释前嫌，合并对抗OpenAI，被业内称为“谷歌复仇联盟”。然而，直到年底的12月7日，谷歌最强大模型Gemini才姗姗来迟，尽管效果惊艳却令市场有些意兴阑珊。2024年1月31日，谷歌最新财报显示其收入亮眼，却因AI方面进展不及预期市值<strong>一夜蒸发超1000亿美元</strong>。</p><p>然而，2024年2月一来到，谷歌的状态来了个180度大转弯，攒了一年的大招接二连三地释放，试图用<strong>强大的Gemini大模型矩阵</strong>证明，其是被严重低估的。</p><p>值得一提的是，谷歌还有另一张王牌是<strong>自研芯片</strong>，有望成为其与OpenAI抗衡的有力底牌。2023年8月，谷歌云发布最新云端AI芯片TPU v5e，TPU被视作全球AI芯片霸主英伟达GPU的劲敌。</p><p>据半导体研究和咨询公司SemiAnalysis的分析师曝料，谷歌拥有的算力资源比OpenAI、Meta、亚马逊、甲骨文和CoreWeave加起来还要多，其下一代大模型Gemini已经开始在新的TPUv5 Pod上进行训练，算力达到GPT-4的5倍，基于其目前的基础设施建设情况，到明年年底可能达到20倍。</p><h2>04.结语：谷歌再放大招，拳打OpenAI，脚踢Meta</h2><p>从2023年12月发布Gemini多模态大模型，到2024年2月连放Gemini Ultra免费版、Gimini 1.5、Gemini技术开源三个大招，谷歌的大模型矩阵逐渐清晰，从闭源和开源两大路线对OpenAI打响复仇战，也向推出开源模型Llama 2的Meta宣战。</p><p>当下，OpenAI的文生视频大模型Sora风头正盛。实际上，谷歌已于2023年12月推出了用于零样本视频生成的大型语言模型VideoPoet，可在单个大模型中无缝集成了多种视频生成功能。谷歌在文生视频领域的储备想必也深，可以预测后续和OpenAI有得一打，而压力也就此给到了国内的AI企业。</p><p>本文来自微信公众号<a href="http://mp.weixin.qq.com/s?__biz=MzA4MTQ4NjQzMw==&amp;mid=2652767448&amp;idx=1&amp;sn=c38238e300006d0ffdfa4ea7146cf6a9&amp;chksm=856b562d64f0dbca9293ad3392be10c1e47066d860b0844fb02e41a3b26235e10e900418590c&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“智东西”（ID：zhidxcom）</a>，作者：香草 李水青，编辑：李水青，36氪经授权发布。</p>
</div></div>
</div>

<div>
[原文](https://www.36kr.com/p/2658586582778119)
</div>

