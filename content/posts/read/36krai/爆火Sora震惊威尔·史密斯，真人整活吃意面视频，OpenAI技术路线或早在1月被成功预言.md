---
title: '爆火Sora震惊威尔·史密斯，真人整活吃意面视频，OpenAI技术路线或早在1月被成功预言'
categories: ['36krai']
date: Tue, 20 Feb 2024 08:31:39 GMT
lastmod: Tue, 20 Feb 2024 08:31:39 GMT
author: ["g0f"]
tags:
- read
draft: false 
comments: true
reward: true 
mermaid: true 
showToc: true 
TocOpen: true 
hidemeta: false 
disableShare: true 
showbreadcrumbs: true 
cover:
    image: "/hugo-logo-wide.svg"
    alt: 果粉圈
    relative: false
---

<div>

<div> OpenAI、Sora、Transformer、AI视频、技术发展<br/>
OpenAI推出了名为Sora的新一代AI视频生成模型，采用了Transformer框架和LLM路线，在技术路线上引领全球。Sora的出现一方面得益于前阿里的AI专家预言了这一技术方向，另一方面吸取了其他机构和名校的贡献，包括斯坦福的技术研究和NÜWA模型的设计思路。Sora的技术应用超越了传统的Diffusion Model模型，通过时空Patch的应用实现了图像和视频数据的强大生成能力。OpenAI的成功不仅得益于技术的创新，也依赖于大量的资金支持，这一点在全球范围内都得到了认可。总之，Sora的诞生代表了AI视频领域的一次革命，标志着AI技术发展的新的里程碑。<br/>总结: <br/>OpenAI推出了名为Sora的新一代AI视频生成模型，采用了Transformer框架和LLM路线，在技术路线上引领全球。Sora的出现一方面得益于前阿里的AI专家预言了这一技术方向，另一方面吸取了其他机构和名校的贡献，包括斯坦福的技术研究和NÜWA模型的设计思路。Sora的技术应用超越了传统的Diffusion Model模型，通过时空Patch的应用实现了图像和视频数据的强大生成能力。OpenAI的成功不仅得益于技术的创新，也依赖于大量的资金支持，这一点在全球范围内都得到了认可。总之，Sora的诞生代表了AI视频领域的一次革命，标志着AI技术发展的新的里程碑。 <div>
<p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_ea066c1af2504e3c82633918772c18ea@46958_oswg221364oswg1030oswg408_img_png?x-oss-process=image/quality,q_100/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p><strong>【导读】</strong>威尔·史密斯的这段视频，把全网都骗了！其实Sora的技术路线，早已被人预言了。李飞飞去年就用Transformer做出了逼真的视频。但只有OpenAI大力出奇迹，跑在了所有人前面。</p><p>今天，全体AI社区都被威尔·史密斯发出的这段视频震惊了！ </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_a613eda6623244179d0b610735300864@000000_oswg682348oswg1080oswg1396_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>你以为，上面是一年前的AI视频，下面是如今的AI视频？ </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_98a6a00897bf4275879b14326eafdf79@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>错！这个所谓AI生成的视频，其实正是威尔史密斯本人！ </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_24f6bfb2c9df449788deaad320898717@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>威尔·史密斯吃意面这个「图灵测试」，曾让Runway、Pika等屡屡翻车。 </p><p>Runway生成的，是这样的——</p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_7d36aad564744195810433967e067a61@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_6cf9b0384ad046cc82c13ec6903decc1@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_125015feb67c4d039e41f544dec63478@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>但如今，Sora已经做到了逼真似真人、毫无破绽，所以才让威尔史密斯成功骗过了大众，这太可怕了！ </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_8b2c19c54c1a4198997ac72baa6b9cab@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><h2><strong>Sora的出现，其实在今年1月就已被人预言</strong></h2><p>1月5日，一位前阿里的AI专家表示—— </p><blockquote><p>我认为，Transformer框架和LLM路线，将是AI视频的一个突破口和新范式，它将使AI视频更加连贯、一致，并且时长更长。目前的Diffusion+Unet路线（如Runway、Pika等），只是暂时的解决方案。 </p></blockquote><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_2de79833428f4a4e97511b75d58c9897@000000_oswg89507oswg1021oswg361_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>无独有偶，斯坦福学者李飞飞在去年年底，就用Transformer就做出了逼真的视频。 </p><p>而马毅教授也表示，自己团队去年在NeurIPS一篇论文中也已经证实，用Transformer可以实现diffusion和denosing。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_a8432da26b4e4944b40a0bcbbde53e75@000000_oswg312047oswg1080oswg558_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>马毅团队提出：假设数据分布是mixed Gaussians，那Transformer blocks就是在实现diffusion/扩散和denoising/压缩 </p><p>能想到Sora技术路线的，肯定不止一个人。可是全世界第一个把Sora做出来的，就是OpenAI。 </p><p>OpenAI为何总能成功？无他，唯手快尔。 </p><h2><strong>Runway和Pika「点歪」的科技树，被OpenAI掰正了</strong></h2><p>在此之前，Runway、Pika等AI视频工具吸引了不少聚光灯。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_35386a11ef214e57bddbfa669e90a849@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>而OpenAI的Sora，不仅效果更加真实，就是把Transformer对前后文的理解和强大的一致性，发挥得淋漓尽致。 </p><p>这个全新的科技树，可真是够震撼的。 </p><p>不过我们在开头也可以看到，OpenAI并不是第一个想到这个的人。Transformer框架+LLM路线这种新范式，其实早已有人想到了。 </p><p>就如同AI大V「阑夕」所言，OpenAI用最简单的话，把最复杂的技术讲清楚了—— </p><p>「图片只是单帧的视频。」 </p><p>科技行业这种从容的公共表达，真是前所未见，令人醍醐灌顶。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_5e0f6c45ba60482d9c0102c4e0e9eaf8@000000_oswg256852oswg690oswg614_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>「阑夕」指出，「图片只是单帧的视频」的妙处就在于，图片的创建不会脱离时间轴而存在，Sora实际上是提前给视频写了脚本的。 </p><p>甚至无论用户怎样Prompt，Sora AI都有自己的构图思维。 </p><p>而这，就是困住Runway、Pika等公司最大的问题。 </p><p>它们的思路，基本都是基于一张图片来让AI去想象，完成延伸和填补，从而叠加成视频。比拼的是谁家的AI更能理解用户想要的内容。 </p><p>因此，这些AI视频极易发生变形，如何保持一致性成了登天般的难题。 </p><p>Diffusion Model这一局，是彻底输给Transformer了。 </p><p>ChatGPT故事再次重演，Sora其实站在谷歌的肩膀上——</p><p>让我们深入扒一扒，Sora是站在哪些前人的肩膀上。 </p><p>简而言之，最大创新Patch的论文，是谷歌发表的。 </p><p>Diffusion Transformer的论文，来自William Peebles和谢赛宁。 </p><p>此外，Meta等机构、UC伯克利等名校皆有贡献。 </p><h3><strong>William Peebles和谢赛宁提出的框架</strong></h3><p>纽约大学计算机系助理教授谢赛宁在分析了Sora的技术报告后表示，Sora应该是基于自己和William Peebles提出的框架设计而成。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_450a1cf23fce4ce1b1066c35861855ef@000000_oswg212453oswg1044oswg856_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>这篇提出了Sora基础架构的论文，去年被ICCV收录。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_cee7a22ecf4644009d6a2aa87a07c010@000000_oswg1358824oswg1080oswg990_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>论文地址：https://arxiv.org/abs/2212.09748 </p><p>随后，William Peebles加入了OpenAI，领导了开发Sora的技术团队。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_b26e20dab56e44f894787e8eb209f7a5@000000_oswg1472355oswg1058oswg1199_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>图灵三巨头之一、Meta AI主管LeCun，也转发了谢赛宁的帖子表示认可。 </p><p>巧合的是，谢赛宁是LeCun的前FAIR同事、现纽约大学同事，William Peebles是LeCun的前伯克利学生、现任OpenAI工程师。AI果然是个圈。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_926bc75e657742e392f728febe6c2850@000000_oswg995640oswg690oswg2081_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>最近，谢赛宁对说自己是Sora作者的说法进行了辟谣 </p><h3><strong>CVPR「有眼不识泰山」，拒掉Sora基础论文</strong></h3><p>有趣的是，Diffusion Transformer这篇论文曾因「缺乏创新性」被CVPR 2023拒收，后来才被ICCV2003接收。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_836dbc7498ca4fb39f5667b6d047cdac@000000_oswg64186oswg693oswg460_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>谢赛宁表示，他们在DIT项目没有创造太多的新东西，但是两个方面的问题：简单性和可扩展性。这可能就是Sora为什么要基于DIT构建的主要原因。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_1953369a4d4441c880b6a4cb29f5c4e8@000000_oswg224306oswg1080oswg564_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>此前，生成模型的方法包括GAN、自回归、扩散模型。它们都有各自的优势和局限性。 </p><p>而Sora引入的，是一种全新的范式转变——新的建模技术和灵活性，可以处理各种时间、纵横比和分辨率。 </p><p>Sora所做的，是把Diffusion和Transformer架构结合在一起，创建了diffusion transformer模型。 </p><p>这也即是OpenAI的创新之处。 </p><h3><strong>时空Patch是谷歌的创新</strong></h3><p>时空Patch，是Sora创新的核心。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_ade328bc74f4462e8c3a296531c70a19@000000_oswg291665oswg1080oswg574_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>它建立在Google DeepMind早期对NaViT和ViT（视觉Transformer）的研究之上。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_6a0b2cd1be7b4288a7a2dfae170d6568@000000_oswg152167oswg1080oswg375_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>论文地址：https://arxiv.org/abs/2307.06304 </p><p>而这项研究，又是基于一篇2021年的论文「An Image is Worth 16x16 Words」。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_4f0788cfc53448fda17a0f6fb41e547b@000000_oswg171719oswg1080oswg387_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>论文地址：https://arxiv.org/abs/2010.11929 </p><p>传统上，对于视觉Transformer，研究者都是使用一系列图像Patch来训练用于图像识别的Transformer模型，而不是用于语言Transformer的单词。 </p><p>这些Patch，能使我们能够摆脱卷积神经网络进行图像处理。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_83251a20a46c47cc92f40140534b0bd3@000000_oswg81842oswg1080oswg203_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>然而，视觉Transforemr对图像训练数据的限制是固定的，这些数据的大小和纵横比是固定的，这就限制了质量，并且需要大量的图像预处理。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_3450b8384aee4f2eb755235a8a9d36e0@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>而通过将视频视为Patch序列，Sora保持了原始的纵横比和分辨率，类似于NaViT对图像的处理。 </p><p>这种保存，对于捕捉视觉数据的真正本质至关重要！ </p><p>通过这种方法，模型能够从更准确的世界表示中学习，从而赋予Sora近乎神奇的准确性。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_68ef504fcd70475f9e18a4f19f929a11@000000_oswg258857oswg1080oswg232_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">时空Patch的可视化 </p><p>谷歌Patch的论文，发表于2021年。3年后，OpenAI基于这项技术，做出了Sora。 </p><p>这段历史看起来是不是有点眼熟？简直就像「Attention Is All You Need」的历史重演。 </p><p>2017年6月12日，8位谷歌研究人员发表了Attention is All You Need，大名鼎鼎的Transformer横空出世。 </p><p>它的出现，让NLP变了天，成为自然语言领域的主流模型。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_50590220078849cfae661ff41623c32a@000000_oswg133497oswg960oswg516_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>论文地址：https://arxiv.org/pdf/1706.03762.pdf </p><p>它完全摒弃了递归结构，依赖注意力机制，挖掘输入和输出之间的关系，进而实现了并行计算。 </p><p>在谷歌看来，Transformer是一种语言理解的新型神经网络架构。不过它当初被设计出来，是为了解决翻译问题。 </p><p>而后来，Transformer架构被OpenAI拿来发扬光大，成为ChatGPT这类LLM的核心。 </p><p>2022年，OpenAI用谷歌17年发表的Transformer做出ChatGPT。 </p><p>2024年，OpenAI用谷歌21年发表的Patch做出Sora。 </p><p>这也让人不由感慨：诚如《为什么伟大不能被计划》一书中所言，伟大的成就与发明，往往是偏离最初计划的结果。 </p><p>前人的无心插柳，给后人的成功做好了奠基石，而一条成功的道路是如何踏出的，完全是出于偶然。 </p><h3><strong>Meta微软UC伯克利斯坦福MIT亦有贡献</strong></h3><p>此外，从Sora参考文献中可以看出，多个机构和名校都对Sora做出了贡献。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_9665176430f94e4fbc856155123820a5@000000_oswg276214oswg1080oswg993_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>比如，用Transformer做扩散模型的去噪骨干这个方法，早已被斯坦福学者李飞飞证明。 </p><p>在去年12月，李飞飞携斯坦福联袂谷歌，用Transformer生成了逼真视频。 </p><p>生成的效果可谓媲美Gen-2比肩Pika，当时许多人激动地感慨——2023年已成AI视频元年，谁成想2024一开年，OpenAI新的震撼就来了！ </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_16a498bcc72a4938875fa058c573b436@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>李飞飞团队做的，是一个在共享潜空间中训练图像和视频生成的，基于Transformer的扩散模型。 </p><p>史上首次，AI学者证明了：Transformer架构可以将图像和视频编码到一个共享的潜空间中！ </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_cda9f7d24f914602b2cc296a5cb93d70@000000_oswg762250oswg1080oswg858_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>论文：https://walt-video-diffusion.github.io/assets/W.A.L.T.pdf </p><p>MSRA和北大联合团队提出的统一多模态预训练模型——NÜWA（女娲），也为Sora做出了贡献。 </p><p>此前的多模态模型要么只能处理图像，要么只能处理视频，而NÜWA则可以为各种视觉合成任务，生成新的图像和视频数据。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_71e67f6991704127b03b8dc16da3a6b6@000000_oswg77713oswg1080oswg178_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>项目地址：https://github.com/microsoft/NUWA </p><p>为了在不同场景下同时覆盖语言、图像和视频，团队设计了一个三维变换器编码器-解码器框架。 </p><p>它不仅可以处理作为三维数据的视频，还可以适应分别作为一维和二维数据的文本和图像。 </p><p>在8个下游任务中，NÜWA都取得了新的SOTA，在文本到图像生成中的表现，更是直接超越了DALL-E。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_8154ef11f84549f193bc3b2ec249f5e1@000000_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p class="img-desc">NÜWA模型支持的8种典型视觉生成任务 </p><p>草蛇灰线，伏脉千里。踩在前人的肩膀上，通过敏锐的直觉和不眠不休的高强度工作，OpenAI的研究者就这样点对了科技树。 </p><h2><strong>大力出奇迹的时候到了，不拿出一百亿美金的大厂就会out</strong></h2><p>当然，还有一点不得不承认的是：OpenAI能做出Sora，也是因为背后大量的资金支持。 </p><p>没有资金，就没有数据和算力。即使点对了科技树也无法验证。 </p><p>可以说，Sora是另一个建立在Transformer上的暴力美学。 </p><p>现在，芯片+AI是人类有史以来最大的科技浪潮。 </p><p>不拿出100亿美金的大厂，就要掉队了。 </p><p class="image-wrapper"><img src="https://img.36krcdn.com/hsossms/20240220/v2_bad9b4ce85ca458995d4c6bbcca3da87@000000_oswg186932oswg1080oswg1101_img_000?x-oss-process=image/format,jpg/interlace,1/format,jpg/interlace,1/format,jpg/interlace,1"/></p><p>国内这边，格局又会怎样变换？让我们拭目以待。 </p><p>参考资料： </p><p>https://weibo.com/1727858283/O1isjz6aw </p><p>https://openai.com/research/video-generation-models-as-world-simulators </p><p>https://weibo.com/3235040884/O19wnxB9Y </p><p>本文来自微信公众号<a href="http://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&amp;mid=2652445827&amp;idx=1&amp;sn=beec7b657600873a232218c14fafaf7b&amp;chksm=f0dcc952d5cdbf43abc6ea56ca926249c3f2f3a744d90ae84fa9ab724616c27c0d47c68c5dc2&amp;scene=0&amp;xtrack=1#rd" rel="noopener noreferrer nofollow" target="_blank">“新智元”（ID：AI_era）</a>，编辑：Aeneas 好困，36氪经授权发布。</p>
</div></div>
</div>

<div>
[原文](https://www.36kr.com/p/2656422604112134)
</div>

